{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AWL_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0HKAMykB17fe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Attention-with-logits, a simplistic but effective approach to use document-level sentiment labels to train sentence-level sentiment classifiers*"
      ]
    },
    {
      "metadata": {
        "id": "LdhzrxiW2fQB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sentence level sentiment classification is a very important task in review mining. To train such sentence-level sentiment classifiers, we need to prepare datasets of review sentences labelled with their ground-truth sentiment labels. However, the sentence-level labelling is very labor-intensive. A popular way to get around the manual labelling efforts is using reviews, instead of review sentences as the training data. In this approach, each review is treated as a single very long sentence, and the rating of the review is treated as the sentiment label for the long sentence.\n",
        "\n",
        "Howerer, this approach suffers some disadvantages:\n",
        "\n",
        "1. there exists a high possibility that the sentiments of indiviual sentences in a review are very different from the  sentiment indicated by the rating. Using the coarse-grained review-level rating as the training label may misrepresent the sentiments of individual sentences in the training process\n",
        "\n",
        "2. a review may consist of hudrends of words. Some deep learning models, such as LSTM, GRU, or CNN cannot deal with such long word sequences. In practice, we usually have to truncate long sentences to limit the length of each training example within 500. This will lead to severe loss of information.\n",
        "\n",
        "\n",
        "We proposed an end-to-end weakly supervised approach, called 'attention-with-logits', that takes individual sentences as the inputs, but uses the review-level ratings as the supervision signal. In the proposed approach, individual sentences are fed into a neural network model, and the resulting sentence-level logit vectors are combined into document-level logit vectors with a LSTM-based attention meachnism. The document-level logit vectors and the document-level ratings are used to define the loss of the neural network model. The structure of the approach is shown in the accompying paper."
      ]
    },
    {
      "metadata": {
        "id": "Y924SAsYPMIj",
        "colab_type": "code",
        "outputId": "91b88b38-b171-490d-b31c-5e103a3b8f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install ibm-cos-sdk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ibm-cos-sdk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/d4/7e1fe33819b80d47dafa5c02c905f7acbbdff7e6cca9af668aaeaa127990/ibm-cos-sdk-2.4.4.tar.gz (50kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.4MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-core==2.*,>=2.0.0 (from ibm-cos-sdk)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/72/99afcdf6b92840d47c8765533ef6093e43059424e3b35dd31049f09c8d7a/ibm-cos-sdk-core-2.4.4.tar.gz (1.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.1MB 20.9MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.*,>=2.0.0 (from ibm-cos-sdk)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/44/c71a4595d311772953775b3588307ac8dd5a36501b3dfda6324173b963cc/ibm-cos-sdk-s3transfer-2.4.4.tar.gz (214kB)\n",
            "\u001b[K    100% |████████████████████████████████| 215kB 26.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) (0.9.4)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) (0.14)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) (1.22)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) (1.11.0)\n",
            "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e5/dc/54/f601cc8263513665653fbf124f6989dcbaeb218fcf1a8fd4d1\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/43/73/3e/79ee45c864491743309c46837d617c0550e58978659b8f742e\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/45/52/14/5239d330c7bd818043a3c578329f1ecff4f1d09694b4c7aa41\n",
            "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
            "Installing collected packages: ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk\n",
            "Successfully installed ibm-cos-sdk-2.4.4 ibm-cos-sdk-core-2.4.4 ibm-cos-sdk-s3transfer-2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L8BGpntXO1s8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import json\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#import os\n",
        "#from nltk.tokenize import sent_tokenize\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np \n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "#import nltk\n",
        "#import re\n",
        "import collections\n",
        "#import gensim\n",
        "import pandas as pd \n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import ibm_boto3\n",
        "from ibm_botocore.client import Config\n",
        "bucket_name = 'aclawl'\n",
        "filename='w2v.csv'\n",
        "credentials = {\n",
        "  \"apikey\": \"eOiK2XTQ9ryhfjzQBBYCz2bw7jASG00KD132KLtcjIIY\",\n",
        "  \"cos_hmac_keys\": {\n",
        "    \"access_key_id\": \"08598b29228a4a2bb8b3b16ab9c6449a\",\n",
        "    \"secret_access_key\": \"ba0fdde59ab475b5484132512c77499f0feb8e6046732f46\"\n",
        "  },\n",
        "  \"endpoints\": \"https://control.cloud-object-storage.cloud.ibm.com/v2/endpoints\",\n",
        "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:cloud-object-storage:global:a/fb3c25e92a5a47319ba409b53b2c431e:0e6438c2-bd8f-41b2-a5a3-0d11f4b74253::\",\n",
        "  \"iam_apikey_name\": \"auto-generated-apikey-08598b29-228a-4a2b-b8b3-b16ab9c6449a\",\n",
        "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
        "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/fb3c25e92a5a47319ba409b53b2c431e::serviceid:ServiceId-64f51fd7-2ced-4e80-a71a-9bfbea957ce6\",\n",
        "  \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/fb3c25e92a5a47319ba409b53b2c431e:0e6438c2-bd8f-41b2-a5a3-0d11f4b74253::\"\n",
        "}\n",
        "auth_endpoint = 'https://iam.bluemix.net/oidc/token'\n",
        "service_endpoint = 'https://s3.ap-geo.objectstorage.softlayer.net'\n",
        "#service_endpoint='https://s3.ap.cloud-object-storage.appdomain.cloud'\n",
        "resource = ibm_boto3.resource('s3',\n",
        "                      ibm_api_key_id=credentials['apikey'],\n",
        "                      ibm_service_instance_id=credentials['resource_instance_id'],\n",
        "                      ibm_auth_endpoint=auth_endpoint,\n",
        "                      config=Config(signature_version='oauth'),\n",
        "                      endpoint_url=service_endpoint)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MH79CEMuduVu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_pickle_from_obj_storage(bucket_name, key):   # read the data from IBM cloud\n",
        "    obj = resource.Object(bucket_name=bucket_name, key=key).get()\n",
        "    return pickle.load(io.BytesIO(obj['Body'].read()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9kojgSsW6Rr_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 1: The attention-with-logits approach"
      ]
    },
    {
      "metadata": {
        "id": "f94hNGX_eFb7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#get data for AWL. The content of the data for AWL is \n",
        "#all the same as that for the baselines, but organised in different structures\n",
        "all_data=get_pickle_from_obj_storage(bucket_name,'data.pickle')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C5WvCGyW3xyV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_rev_len=10 # the maximum number of setences a review should have \n",
        "max_sen_len=50 # the maximum number of words of a sentences\n",
        "class_num=5 #number of target classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jp1x5bgiv_p0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###class Atten_logits implements the idea of the proposed model. To create an instance, a base model has to be supplied. The parameters of a base model can be learned by fitting the Atten_logits  model"
      ]
    },
    {
      "metadata": {
        "id": "tTPsMmQI7qcf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Atten_logits(tf.keras.Model):\n",
        "  def __init__(self, base_model):\n",
        "    super(Atten_logits, self).__init__()\n",
        "    self.base_model=base_model\n",
        "    self.lstm_atten=tf.keras.layers.LSTM(30,return_sequences=True,name='lstm_atten') #LSTM for attention\n",
        "    self.atten_score=tf.keras.layers.Dense(1,name='atten_score') # map the attention vec to a single score\n",
        "    \n",
        "  def call(self,x): \n",
        "    # x should a list of 2 elements:\n",
        "    #1.the token ids of each sentence. Shape=doc_num*max_rev_len*max_sen_len\n",
        "    #2.the number of sentences each review contains. shape=doc_num*1 \n",
        "    trng_x,trng_emp_sen_msk=x\n",
        "    trng_x=tf.reshape(trng_x, [-1,max_sen_len])\n",
        "    trng_emp_sen_msk=tf.reshape(trng_emp_sen_msk,[-1,max_rev_len,1])   \n",
        "    \n",
        "    base_model_output=self.base_model(trng_x)   #get the output of the base_model\n",
        "    base_model_output = tf.reshape(base_model_output, [-1, max_rev_len, 5],name='output')          \n",
        "    \n",
        "    #compute the attention weight each sentence should be given\n",
        "    atten_vec = self.lstm_atten(base_model_output)    \n",
        "    atten_score=self.atten_score(atten_vec)    \n",
        "    atten_score= tf.exp(atten_score)*trng_emp_sen_msk\n",
        "    atten_score_norm=tf.reshape(tf.reduce_sum(atten_score,axis=1),[-1,1,1])    \n",
        "    attention_weights= atten_score/atten_score_norm  \n",
        "    \n",
        "    #aggregate sen-level logits into doc-level logits \n",
        "    atten_logit=attention_weights * base_model_output \n",
        "    atten_logit=tf.reduce_sum(atten_logit, axis=1)     \n",
        "    \n",
        "    return  tf.nn.softmax(atten_logit,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mduW76Q9mNxj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Create base models. Any neural network based models can be used as the base model, as long as they output logit vectors **"
      ]
    },
    {
      "metadata": {
        "id": "rQTVPnqI72cU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def base_mlp():\n",
        "  new_model=tf.keras.Sequential()\n",
        "  new_embedding=tf.keras.layers.Embedding(embedding_matrix.shape[0],\n",
        "                              embedding_matrix.shape[1],\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=max_sen_len,\n",
        "                              trainable=False,name='embedding')\n",
        "  new_pooling =tf.keras.layers.GlobalAveragePooling1D()\n",
        "  new_dense=tf.keras.layers.Dense(500,activation='relu',name='dense',\n",
        "                                 )\n",
        "  dp_1=tf.keras.layers.Dropout(rate=0.20)\n",
        "  new_dense_1=tf.keras.layers.Dense(50,activation='relu',name='dense_1'\n",
        "                                   )\n",
        "  dp_2=tf.keras.layers.Dropout(rate=0.15)\n",
        "  new_dense_2=tf.keras.layers.Dense(5,name='dense_2')\n",
        "  new_model.add(new_embedding)\n",
        "  new_model.add(new_pooling)\n",
        "  new_model.add(new_dense)\n",
        "  new_model.add(dp_1)\n",
        "  new_model.add(new_dense_1)\n",
        "  new_model.add(dp_2)\n",
        "  new_model.add(new_dense_2)\n",
        "  new_model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
        "  return new_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A9K1eohu8OoZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def base_seq(seq_type='LSTM'):\n",
        "  new_model=tf.keras.Sequential()\n",
        "  new_embedding=tf.keras.layers.Embedding(embedding_matrix.shape[0],\n",
        "                              embedding_matrix.shape[1],\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=max_sen_len,\n",
        "                              trainable=False,name='embedding')\n",
        "  if seq_type=='LSTM':\n",
        "    new_rnn = tf.keras.layers.LSTM(300,\n",
        "                             kernel_regularizer=tf.keras.regularizers.l2(0.0005),\n",
        "                             #recurrent_regularizer=tf.keras.regularizers.l2(0.001),\n",
        "                             dropout=0.2,\n",
        "                             recurrent_dropout=0.2,name='lstm') \n",
        "  else:\n",
        "    new_rnn = tf.keras.layers.GRU(200,\n",
        "                             kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
        "                             recurrent_regularizer=tf.keras.regularizers.l2(0.001),\n",
        "                             dropout=0.1,\n",
        "                             recurrent_dropout=0.1,name='lstm')  \n",
        "  \n",
        "  new_dense_1=tf.keras.layers.Dense(50,activation='tanh',name='dense_1')\n",
        "  drop_1=tf.keras.layers.Dropout(rate=0.3)\n",
        "  new_dense_2=tf.keras.layers.Dense(5,name='dense_2')\n",
        "  new_model.add(new_embedding)\n",
        "  new_model.add(new_rnn)\n",
        "  new_model.add(new_dense_1)\n",
        "  new_model.add(drop_1)\n",
        "  \n",
        "  new_model.add(new_dense_2)\n",
        "  \n",
        "  new_model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
        "  return new_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kYnu0crUmvLx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_sen_content=all_data['trn_sen_content'] # get the sen tokens of each review in the training data. Shape=doc_num*max_rev_len*max_sen_len\n",
        "##### Assuming max_rev_len=10, however, if a review has only 2 sentences, then we need create 8 dummy sentences with content of all 0s\n",
        "trn_rating_oh=all_data['trn_rating_oh']# get the rating of each document in the one-hot encoding format\n",
        "trn_rev_len=all_data['trn_rev_len'] # get the actual number of sentences in each review\n",
        "\n",
        "embedding_matrix=all_data['embedding_matrix']\n",
        "\n",
        "val_sen_content=all_data['val_x'] \n",
        "val_rating_oh=all_data['val_y']\n",
        "val_rev_len=all_data['val_len']\n",
        "\n",
        "tst=all_data['tst']  # get the sen tokens for test\n",
        "tst_labels=all_data['tst_labels']  # get the ground truth labels of the test sentences\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBfMAdJQNxAI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "####create masks to not let the dummy sentences play roles when computing the attention weights\n",
        "def mask_based_len(rev_len):\n",
        "  masks=[]\n",
        "  for i in rev_len:\n",
        "    masks.append([1]*i+[0]*(max_rev_len-i))\n",
        "  return np.array(masks)\n",
        "\n",
        "trng_epty_sen_mask=mask_based_len(trn_rev_len.squeeze())\n",
        "val_epty_sen_mask=mask_based_len(val_rev_len.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AgYkLN-x02OV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train a LSTM classifiers with the proposed approach"
      ]
    },
    {
      "metadata": {
        "id": "taEAvkWMhnZx",
        "colab_type": "code",
        "outputId": "3a20fdb1-5a91-48cd-cb12-e2208a8cf47f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "base_model=base_seq()\n",
        "awl_model=Atten_logits(base_model)\n",
        "awl_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.005),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "#history=model.fit([trn_sen_content,trn_rev_len], trn_rating_oh,validation_data=([val_x,val_len],val_y), batch_size=128, epochs=80)\n",
        "#my_model.predict([trn_sen_content[0:30],epty_sen_mask[0:30]])\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',patience=10,verbose=1)\n",
        "#my_model.fit(trn_sen_content, trn_rating_oh, validation_data=[val_x,val_y], callbacks=[es],batch_size=128, epochs=200,shuffle=False)\n",
        "awl_model.fit([trn_sen_content,trng_epty_sen_mask],trn_rating_oh,\n",
        "             validation_data=([val_sen_content,val_epty_sen_mask],val_rating_oh),\n",
        "             batch_size=64,callbacks=[es],epochs=70,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11535 samples, validate on 1500 samples\n",
            "Epoch 1/70\n",
            "11535/11535 [==============================] - 49s 4ms/sample - loss: 1.4657 - acc: 0.3824 - val_loss: 1.3217 - val_acc: 0.4327\n",
            "Epoch 2/70\n",
            "11535/11535 [==============================] - 48s 4ms/sample - loss: 1.3181 - acc: 0.4318 - val_loss: 1.2638 - val_acc: 0.4480\n",
            "Epoch 3/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.2443 - acc: 0.4720 - val_loss: 1.2140 - val_acc: 0.4740\n",
            "Epoch 4/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.2232 - acc: 0.4869 - val_loss: 1.1747 - val_acc: 0.4953\n",
            "Epoch 5/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1824 - acc: 0.5113 - val_loss: 1.1319 - val_acc: 0.5167\n",
            "Epoch 6/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1742 - acc: 0.5171 - val_loss: 1.1078 - val_acc: 0.5420\n",
            "Epoch 7/70\n",
            "11535/11535 [==============================] - 48s 4ms/sample - loss: 1.1549 - acc: 0.5277 - val_loss: 1.0970 - val_acc: 0.5507\n",
            "Epoch 8/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1517 - acc: 0.5263 - val_loss: 1.1118 - val_acc: 0.5320\n",
            "Epoch 9/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1392 - acc: 0.5313 - val_loss: 1.1058 - val_acc: 0.5480\n",
            "Epoch 10/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1368 - acc: 0.5266 - val_loss: 1.1288 - val_acc: 0.5367\n",
            "Epoch 11/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1343 - acc: 0.5351 - val_loss: 1.1234 - val_acc: 0.5327\n",
            "Epoch 12/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1233 - acc: 0.5436 - val_loss: 1.1238 - val_acc: 0.5360\n",
            "Epoch 13/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1229 - acc: 0.5324 - val_loss: 1.1275 - val_acc: 0.5333\n",
            "Epoch 14/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1247 - acc: 0.5449 - val_loss: 1.1283 - val_acc: 0.5313\n",
            "Epoch 15/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1111 - acc: 0.5436 - val_loss: 1.1185 - val_acc: 0.5353\n",
            "Epoch 16/70\n",
            "11535/11535 [==============================] - 48s 4ms/sample - loss: 1.1079 - acc: 0.5437 - val_loss: 1.1295 - val_acc: 0.5333\n",
            "Epoch 17/70\n",
            "11535/11535 [==============================] - 47s 4ms/sample - loss: 1.1077 - acc: 0.5386 - val_loss: 1.1372 - val_acc: 0.5340\n",
            "Epoch 00017: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9ec185a908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "FuyRL4F82nzH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model,tst,tst_labels):\n",
        "  predictions=np.argmax(model.predict(tst),axis=1)\n",
        "  predictions=np.squeeze(predictions).tolist()\n",
        "  converted_preds=[]\n",
        "  for i in predictions:\n",
        "    if i<2:\n",
        "      converted_preds.append(0)\n",
        "    elif i==2:\n",
        "      converted_preds.append(1)\n",
        "    else:\n",
        "      converted_preds.append(2)\n",
        "\n",
        "  converted_preds=np.array(converted_preds).reshape([-1,1])\n",
        "  print('the accuracy of awl_lstm is %f' %np.mean(tst_labels==converted_preds))\n",
        "  return pd.DataFrame(np.c_[tst_labels,converted_preds],columns=['ground-truth','prediction'])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sHKr6Eb08UY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the performance of the lstm classifier trained with the proposed approach"
      ]
    },
    {
      "metadata": {
        "id": "PjF7GJh_jfwt",
        "colab_type": "code",
        "outputId": "8c684e64-77f3-4b67-b5bb-f58899302074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1986
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate(base_model,tst,tst_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the accuracy of awl_lstm is 0.707165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ground-truth</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>321 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     ground-truth  prediction\n",
              "0               2           2\n",
              "1               2           2\n",
              "2               2           2\n",
              "3               2           2\n",
              "4               2           2\n",
              "5               2           0\n",
              "6               2           2\n",
              "7               2           2\n",
              "8               2           2\n",
              "9               2           2\n",
              "10              2           2\n",
              "11              0           0\n",
              "12              0           1\n",
              "13              2           2\n",
              "14              2           2\n",
              "15              2           2\n",
              "16              1           1\n",
              "17              2           1\n",
              "18              2           2\n",
              "19              2           2\n",
              "20              2           2\n",
              "21              2           2\n",
              "22              2           2\n",
              "23              2           2\n",
              "24              2           2\n",
              "25              2           2\n",
              "26              2           2\n",
              "27              2           2\n",
              "28              0           1\n",
              "29              2           1\n",
              "..            ...         ...\n",
              "291             0           1\n",
              "292             0           1\n",
              "293             0           0\n",
              "294             2           2\n",
              "295             1           2\n",
              "296             2           2\n",
              "297             1           1\n",
              "298             0           1\n",
              "299             0           0\n",
              "300             0           1\n",
              "301             1           1\n",
              "302             1           2\n",
              "303             2           2\n",
              "304             2           2\n",
              "305             2           2\n",
              "306             0           0\n",
              "307             0           0\n",
              "308             0           1\n",
              "309             0           2\n",
              "310             0           0\n",
              "311             0           1\n",
              "312             0           0\n",
              "313             2           1\n",
              "314             2           2\n",
              "315             2           2\n",
              "316             2           2\n",
              "317             2           2\n",
              "318             1           1\n",
              "319             0           1\n",
              "320             2           2\n",
              "\n",
              "[321 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "_lZvsdg51FR_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train a MLP classifier with the proposed approach"
      ]
    },
    {
      "metadata": {
        "id": "ckM62prjpgh6",
        "colab_type": "code",
        "outputId": "a2abcf36-f313-4d64-b54d-413c479512b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1496
        }
      },
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "base_model=base_mlp()\n",
        "#history=model.fit([trn_sen_content,trn_rev_len], trn_rating_oh,validation_data=([val_x,val_len],val_y), batch_size=128, epochs=80)\n",
        "#awl_model.fit(trn_sen_content, trn_rating_oh,batch_size=128, epochs=80,shuffle=False)\n",
        "awl_model=Atten_logits(base_model)\n",
        "awl_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.005),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "#history=model.fit([trn_sen_content,trn_rev_len], trn_rating_oh,validation_data=([val_x,val_len],val_y), batch_size=128, epochs=80)\n",
        "#my_model.predict([trn_sen_content[0:30],epty_sen_mask[0:30]])\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',patience=20,verbose=1)\n",
        "#my_model.fit(trn_sen_content, trn_rating_oh, validation_data=[val_x,val_y], callbacks=[es],batch_size=128, epochs=200,shuffle=False)\n",
        "awl_model.fit([trn_sen_content,trng_epty_sen_mask],trn_rating_oh,\n",
        "             validation_data=([val_sen_content,val_epty_sen_mask],val_rating_oh),\n",
        "             batch_size=64,callbacks=[es],epochs=70,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11535 samples, validate on 1500 samples\n",
            "Epoch 1/70\n",
            "11535/11535 [==============================] - 8s 726us/sample - loss: 1.5626 - acc: 0.2691 - val_loss: 1.3802 - val_acc: 0.3680\n",
            "Epoch 2/70\n",
            "11535/11535 [==============================] - 7s 639us/sample - loss: 1.3728 - acc: 0.3771 - val_loss: 1.2592 - val_acc: 0.4353\n",
            "Epoch 3/70\n",
            "11535/11535 [==============================] - 8s 654us/sample - loss: 1.2736 - acc: 0.4261 - val_loss: 1.2164 - val_acc: 0.4487\n",
            "Epoch 4/70\n",
            "11535/11535 [==============================] - 7s 646us/sample - loss: 1.2309 - acc: 0.4455 - val_loss: 1.1818 - val_acc: 0.4493\n",
            "Epoch 5/70\n",
            "11535/11535 [==============================] - 8s 654us/sample - loss: 1.2012 - acc: 0.4577 - val_loss: 1.1534 - val_acc: 0.4687\n",
            "Epoch 6/70\n",
            "11535/11535 [==============================] - 7s 645us/sample - loss: 1.1714 - acc: 0.4752 - val_loss: 1.1329 - val_acc: 0.4880\n",
            "Epoch 7/70\n",
            "11535/11535 [==============================] - 8s 660us/sample - loss: 1.1461 - acc: 0.4894 - val_loss: 1.1204 - val_acc: 0.5000\n",
            "Epoch 8/70\n",
            "11535/11535 [==============================] - 8s 663us/sample - loss: 1.1289 - acc: 0.4974 - val_loss: 1.1074 - val_acc: 0.5067\n",
            "Epoch 9/70\n",
            "11535/11535 [==============================] - 8s 651us/sample - loss: 1.1138 - acc: 0.5045 - val_loss: 1.0939 - val_acc: 0.5100\n",
            "Epoch 10/70\n",
            "11535/11535 [==============================] - 8s 651us/sample - loss: 1.1050 - acc: 0.5116 - val_loss: 1.0968 - val_acc: 0.5133\n",
            "Epoch 11/70\n",
            "11535/11535 [==============================] - 7s 645us/sample - loss: 1.0920 - acc: 0.5161 - val_loss: 1.0936 - val_acc: 0.5127\n",
            "Epoch 12/70\n",
            "11535/11535 [==============================] - 7s 645us/sample - loss: 1.0849 - acc: 0.5138 - val_loss: 1.0842 - val_acc: 0.5193\n",
            "Epoch 13/70\n",
            "11535/11535 [==============================] - 7s 641us/sample - loss: 1.0776 - acc: 0.5228 - val_loss: 1.0855 - val_acc: 0.5193\n",
            "Epoch 14/70\n",
            "11535/11535 [==============================] - 8s 669us/sample - loss: 1.0659 - acc: 0.5225 - val_loss: 1.0834 - val_acc: 0.5173\n",
            "Epoch 15/70\n",
            "11535/11535 [==============================] - 8s 660us/sample - loss: 1.0605 - acc: 0.5267 - val_loss: 1.0780 - val_acc: 0.5273\n",
            "Epoch 16/70\n",
            "11535/11535 [==============================] - 7s 649us/sample - loss: 1.0511 - acc: 0.5315 - val_loss: 1.0797 - val_acc: 0.5213\n",
            "Epoch 17/70\n",
            "11535/11535 [==============================] - 8s 666us/sample - loss: 1.0447 - acc: 0.5355 - val_loss: 1.0758 - val_acc: 0.5207\n",
            "Epoch 18/70\n",
            "11535/11535 [==============================] - 8s 669us/sample - loss: 1.0393 - acc: 0.5382 - val_loss: 1.0862 - val_acc: 0.5187\n",
            "Epoch 19/70\n",
            "11535/11535 [==============================] - 8s 657us/sample - loss: 1.0330 - acc: 0.5373 - val_loss: 1.0841 - val_acc: 0.5193\n",
            "Epoch 20/70\n",
            "11535/11535 [==============================] - 8s 669us/sample - loss: 1.0291 - acc: 0.5410 - val_loss: 1.0895 - val_acc: 0.5187\n",
            "Epoch 21/70\n",
            "11535/11535 [==============================] - 8s 662us/sample - loss: 1.0285 - acc: 0.5389 - val_loss: 1.0839 - val_acc: 0.5127\n",
            "Epoch 22/70\n",
            "11535/11535 [==============================] - 8s 658us/sample - loss: 1.0141 - acc: 0.5469 - val_loss: 1.0735 - val_acc: 0.5213\n",
            "Epoch 23/70\n",
            "11535/11535 [==============================] - 8s 670us/sample - loss: 1.0169 - acc: 0.5448 - val_loss: 1.0898 - val_acc: 0.5260\n",
            "Epoch 24/70\n",
            "11535/11535 [==============================] - 7s 639us/sample - loss: 1.0088 - acc: 0.5473 - val_loss: 1.0883 - val_acc: 0.5187\n",
            "Epoch 25/70\n",
            "11535/11535 [==============================] - 8s 656us/sample - loss: 1.0037 - acc: 0.5480 - val_loss: 1.0822 - val_acc: 0.5180\n",
            "Epoch 26/70\n",
            "11535/11535 [==============================] - 7s 621us/sample - loss: 0.9973 - acc: 0.5524 - val_loss: 1.0886 - val_acc: 0.5113\n",
            "Epoch 27/70\n",
            "11535/11535 [==============================] - 7s 620us/sample - loss: 0.9976 - acc: 0.5560 - val_loss: 1.1161 - val_acc: 0.5147\n",
            "Epoch 28/70\n",
            "11535/11535 [==============================] - 7s 623us/sample - loss: 0.9912 - acc: 0.5556 - val_loss: 1.0867 - val_acc: 0.5200\n",
            "Epoch 29/70\n",
            "11535/11535 [==============================] - 7s 619us/sample - loss: 0.9845 - acc: 0.5596 - val_loss: 1.0973 - val_acc: 0.5300\n",
            "Epoch 30/70\n",
            "11535/11535 [==============================] - 7s 623us/sample - loss: 0.9859 - acc: 0.5601 - val_loss: 1.0962 - val_acc: 0.5353\n",
            "Epoch 31/70\n",
            "11535/11535 [==============================] - 7s 641us/sample - loss: 0.9756 - acc: 0.5645 - val_loss: 1.1020 - val_acc: 0.5253\n",
            "Epoch 32/70\n",
            "11535/11535 [==============================] - 8s 651us/sample - loss: 0.9840 - acc: 0.5541 - val_loss: 1.1265 - val_acc: 0.5227\n",
            "Epoch 33/70\n",
            "11535/11535 [==============================] - 8s 654us/sample - loss: 0.9771 - acc: 0.5659 - val_loss: 1.1100 - val_acc: 0.5127\n",
            "Epoch 34/70\n",
            "11535/11535 [==============================] - 8s 653us/sample - loss: 0.9708 - acc: 0.5660 - val_loss: 1.1027 - val_acc: 0.5193\n",
            "Epoch 35/70\n",
            "11535/11535 [==============================] - 8s 652us/sample - loss: 0.9633 - acc: 0.5717 - val_loss: 1.1061 - val_acc: 0.5140\n",
            "Epoch 36/70\n",
            "11535/11535 [==============================] - 7s 619us/sample - loss: 0.9624 - acc: 0.5730 - val_loss: 1.1075 - val_acc: 0.5300\n",
            "Epoch 37/70\n",
            "11535/11535 [==============================] - 7s 613us/sample - loss: 0.9645 - acc: 0.5703 - val_loss: 1.1173 - val_acc: 0.5180\n",
            "Epoch 38/70\n",
            "11535/11535 [==============================] - 8s 657us/sample - loss: 0.9599 - acc: 0.5729 - val_loss: 1.1041 - val_acc: 0.5227\n",
            "Epoch 39/70\n",
            "11535/11535 [==============================] - 8s 695us/sample - loss: 0.9613 - acc: 0.5730 - val_loss: 1.1222 - val_acc: 0.5127\n",
            "Epoch 40/70\n",
            "11535/11535 [==============================] - 8s 695us/sample - loss: 0.9578 - acc: 0.5717 - val_loss: 1.1148 - val_acc: 0.5233\n",
            "Epoch 41/70\n",
            "11535/11535 [==============================] - 8s 651us/sample - loss: 0.9600 - acc: 0.5679 - val_loss: 1.1301 - val_acc: 0.5107\n",
            "Epoch 42/70\n",
            "11535/11535 [==============================] - 8s 659us/sample - loss: 0.9515 - acc: 0.5772 - val_loss: 1.1388 - val_acc: 0.5153\n",
            "Epoch 00042: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9e8da48780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "yeIZNKp71MW3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Evaluate the performance of the MLP classifier trained with the proposed approach"
      ]
    },
    {
      "metadata": {
        "id": "dvT7GS0EgLTD",
        "colab_type": "code",
        "outputId": "c526e253-0383-4efd-f932-e764090c36f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1986
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate(base_model,tst,tst_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the accuracy of awl_mlp is 0.741433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ground-truth</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>321 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     ground-truth  prediction\n",
              "0               2           2\n",
              "1               2           2\n",
              "2               2           2\n",
              "3               2           2\n",
              "4               2           2\n",
              "5               2           2\n",
              "6               2           2\n",
              "7               2           2\n",
              "8               2           2\n",
              "9               2           2\n",
              "10              2           2\n",
              "11              0           1\n",
              "12              0           1\n",
              "13              2           2\n",
              "14              2           2\n",
              "15              2           2\n",
              "16              1           0\n",
              "17              2           1\n",
              "18              2           2\n",
              "19              2           2\n",
              "20              2           2\n",
              "21              2           2\n",
              "22              2           2\n",
              "23              2           2\n",
              "24              2           2\n",
              "25              2           2\n",
              "26              2           2\n",
              "27              2           2\n",
              "28              0           1\n",
              "29              2           1\n",
              "..            ...         ...\n",
              "291             0           1\n",
              "292             0           0\n",
              "293             0           0\n",
              "294             2           2\n",
              "295             1           2\n",
              "296             2           2\n",
              "297             1           2\n",
              "298             0           1\n",
              "299             0           0\n",
              "300             0           1\n",
              "301             1           1\n",
              "302             1           2\n",
              "303             2           2\n",
              "304             2           2\n",
              "305             2           2\n",
              "306             0           0\n",
              "307             0           0\n",
              "308             0           0\n",
              "309             0           2\n",
              "310             0           1\n",
              "311             0           2\n",
              "312             0           0\n",
              "313             2           2\n",
              "314             2           2\n",
              "315             2           2\n",
              "316             2           2\n",
              "317             2           2\n",
              "318             1           1\n",
              "319             0           1\n",
              "320             2           2\n",
              "\n",
              "[321 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "IAMIqNb06jWe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2: The documents-as-sens approach"
      ]
    },
    {
      "metadata": {
        "id": "L7M96GsmxrVL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######## Read data for docs-as-sens approach\n",
        "######## The content of the data is all the same as that for the AWL,\n",
        "#######  except for they are organised in different structures\n",
        "baseline_max_sen_len=100\n",
        "all_data=get_pickle_from_obj_storage(bucket_name,'base_data.pickle')  \n",
        "trng_content=all_data['all_content']\n",
        "trng_label=all_data['all_rating_oh']\n",
        "val_content=all_data['test_content']\n",
        "val_label=all_data['test_rating_oh']\n",
        "tst=all_data['tst']\n",
        "tst_label=all_data['tst_labels']\n",
        "embedding_matrix=all_data['embedding_matrix']\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EeoGXIV81hO3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train a LSTM model with docs-as-sens approach"
      ]
    },
    {
      "metadata": {
        "id": "Nw0YSU9c73bh",
        "colab_type": "code",
        "outputId": "7d7dbaee-0a9d-41a4-eaf9-23788f4e1aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2312
        }
      },
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential()\n",
        "embedding_layer = tf.keras.layers.Embedding(embedding_matrix.shape[0],\n",
        "                            embedding_matrix.shape[1],\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=baseline_max_sen_len,\n",
        "                            trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(tf.keras.layers.LSTM(128,dropout=0.2,\n",
        "                               recurrent_dropout=0.2,\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
        "                               recurrent_regularizer=tf.keras.regularizers.l2(0.001)))                        \n",
        "model.add(tf.keras.layers.Dense(200,activation='tanh'))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.20))\n",
        "model.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
        "model.add(tf.keras.layers.Dense(5,activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',patience=20,verbose=1)\n",
        "model.fit(trng_content,trng_label,batch_size=128,callbacks=[es],validation_data=[val_content,val_label] ,epochs=70)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10250 samples, validate on 1500 samples\n",
            "Epoch 1/70\n",
            "10250/10250 [==============================] - 27s 3ms/sample - loss: 1.6928 - acc: 0.3476 - val_loss: 1.5168 - val_acc: 0.3967\n",
            "Epoch 2/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.4567 - acc: 0.4159 - val_loss: 1.3628 - val_acc: 0.4473\n",
            "Epoch 3/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.4218 - acc: 0.4181 - val_loss: 1.3757 - val_acc: 0.4260\n",
            "Epoch 4/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.3942 - acc: 0.4380 - val_loss: 1.3189 - val_acc: 0.4607\n",
            "Epoch 5/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.3713 - acc: 0.4509 - val_loss: 1.3157 - val_acc: 0.4500\n",
            "Epoch 6/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.3704 - acc: 0.4466 - val_loss: 1.3458 - val_acc: 0.4340\n",
            "Epoch 7/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.3469 - acc: 0.4540 - val_loss: 1.2916 - val_acc: 0.4700\n",
            "Epoch 8/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.3471 - acc: 0.4533 - val_loss: 1.3070 - val_acc: 0.4780\n",
            "Epoch 9/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.3203 - acc: 0.4720 - val_loss: 1.5057 - val_acc: 0.3933\n",
            "Epoch 10/70\n",
            "10250/10250 [==============================] - 26s 2ms/sample - loss: 1.3422 - acc: 0.4649 - val_loss: 1.3157 - val_acc: 0.4607\n",
            "Epoch 11/70\n",
            "10250/10250 [==============================] - 26s 2ms/sample - loss: 1.3063 - acc: 0.4714 - val_loss: 1.2951 - val_acc: 0.4727\n",
            "Epoch 12/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.3092 - acc: 0.4688 - val_loss: 1.3061 - val_acc: 0.4573\n",
            "Epoch 13/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.2830 - acc: 0.4810 - val_loss: 1.2674 - val_acc: 0.4840\n",
            "Epoch 14/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.2811 - acc: 0.4844 - val_loss: 1.3371 - val_acc: 0.4527\n",
            "Epoch 15/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.2773 - acc: 0.4832 - val_loss: 1.3052 - val_acc: 0.4633\n",
            "Epoch 16/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.2593 - acc: 0.4896 - val_loss: 1.2457 - val_acc: 0.4907\n",
            "Epoch 17/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.2579 - acc: 0.4969 - val_loss: 1.2721 - val_acc: 0.4747\n",
            "Epoch 18/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.2446 - acc: 0.4961 - val_loss: 1.2688 - val_acc: 0.4713\n",
            "Epoch 19/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.2343 - acc: 0.4996 - val_loss: 1.2330 - val_acc: 0.4807\n",
            "Epoch 20/70\n",
            "10250/10250 [==============================] - 27s 3ms/sample - loss: 1.2114 - acc: 0.5120 - val_loss: 1.2143 - val_acc: 0.4780\n",
            "Epoch 21/70\n",
            "10250/10250 [==============================] - 27s 3ms/sample - loss: 1.2036 - acc: 0.5096 - val_loss: 1.2284 - val_acc: 0.4787\n",
            "Epoch 22/70\n",
            "10250/10250 [==============================] - 27s 3ms/sample - loss: 1.2032 - acc: 0.5100 - val_loss: 1.2219 - val_acc: 0.4793\n",
            "Epoch 23/70\n",
            "10250/10250 [==============================] - 27s 3ms/sample - loss: 1.1807 - acc: 0.5147 - val_loss: 1.1944 - val_acc: 0.4947\n",
            "Epoch 24/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1787 - acc: 0.5245 - val_loss: 1.2207 - val_acc: 0.4773\n",
            "Epoch 25/70\n",
            "10250/10250 [==============================] - 27s 3ms/sample - loss: 1.1675 - acc: 0.5265 - val_loss: 1.1918 - val_acc: 0.4940\n",
            "Epoch 26/70\n",
            "10250/10250 [==============================] - 27s 3ms/sample - loss: 1.1769 - acc: 0.5169 - val_loss: 1.1840 - val_acc: 0.4973\n",
            "Epoch 27/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1546 - acc: 0.5340 - val_loss: 1.2127 - val_acc: 0.4960\n",
            "Epoch 28/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1506 - acc: 0.5380 - val_loss: 1.1779 - val_acc: 0.4980\n",
            "Epoch 29/70\n",
            "10250/10250 [==============================] - 27s 3ms/sample - loss: 1.1462 - acc: 0.5361 - val_loss: 1.1779 - val_acc: 0.4980\n",
            "Epoch 30/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1423 - acc: 0.5294 - val_loss: 1.2089 - val_acc: 0.4860\n",
            "Epoch 31/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1405 - acc: 0.5313 - val_loss: 1.1756 - val_acc: 0.4967\n",
            "Epoch 32/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1307 - acc: 0.5382 - val_loss: 1.1751 - val_acc: 0.4987\n",
            "Epoch 33/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.1249 - acc: 0.5479 - val_loss: 1.1789 - val_acc: 0.4987\n",
            "Epoch 34/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.1281 - acc: 0.5441 - val_loss: 1.1866 - val_acc: 0.4887\n",
            "Epoch 35/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1302 - acc: 0.5479 - val_loss: 1.1676 - val_acc: 0.5060\n",
            "Epoch 36/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1221 - acc: 0.5434 - val_loss: 1.1835 - val_acc: 0.4973\n",
            "Epoch 37/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1177 - acc: 0.5522 - val_loss: 1.1754 - val_acc: 0.5127\n",
            "Epoch 38/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1103 - acc: 0.5535 - val_loss: 1.1630 - val_acc: 0.5093\n",
            "Epoch 39/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1107 - acc: 0.5498 - val_loss: 1.1805 - val_acc: 0.5067\n",
            "Epoch 40/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1064 - acc: 0.5611 - val_loss: 1.1923 - val_acc: 0.5033\n",
            "Epoch 41/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1130 - acc: 0.5501 - val_loss: 1.1660 - val_acc: 0.5200\n",
            "Epoch 42/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.1013 - acc: 0.5587 - val_loss: 1.1586 - val_acc: 0.5227\n",
            "Epoch 43/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.0884 - acc: 0.5623 - val_loss: 1.1583 - val_acc: 0.5213\n",
            "Epoch 44/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.0986 - acc: 0.5608 - val_loss: 1.1556 - val_acc: 0.5207\n",
            "Epoch 45/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.0957 - acc: 0.5576 - val_loss: 1.1623 - val_acc: 0.5140\n",
            "Epoch 46/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.0896 - acc: 0.5659 - val_loss: 1.1493 - val_acc: 0.5273\n",
            "Epoch 47/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.0827 - acc: 0.5689 - val_loss: 1.1615 - val_acc: 0.5227\n",
            "Epoch 48/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.0793 - acc: 0.5647 - val_loss: 1.1692 - val_acc: 0.5120\n",
            "Epoch 49/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.0889 - acc: 0.5679 - val_loss: 1.1762 - val_acc: 0.5093\n",
            "Epoch 50/70\n",
            "10250/10250 [==============================] - 26s 2ms/sample - loss: 1.0842 - acc: 0.5654 - val_loss: 1.1681 - val_acc: 0.5187\n",
            "Epoch 51/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.0826 - acc: 0.5658 - val_loss: 1.1651 - val_acc: 0.5200\n",
            "Epoch 52/70\n",
            "10250/10250 [==============================] - 26s 3ms/sample - loss: 1.0828 - acc: 0.5701 - val_loss: 1.1740 - val_acc: 0.5227\n",
            "Epoch 53/70\n",
            "10250/10250 [==============================] - 26s 2ms/sample - loss: 1.0761 - acc: 0.5698 - val_loss: 1.1572 - val_acc: 0.5273\n",
            "Epoch 54/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.0715 - acc: 0.5792 - val_loss: 1.1566 - val_acc: 0.5220\n",
            "Epoch 55/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.0802 - acc: 0.5675 - val_loss: 1.1880 - val_acc: 0.5140\n",
            "Epoch 56/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.0764 - acc: 0.5702 - val_loss: 1.1723 - val_acc: 0.5193\n",
            "Epoch 57/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.0796 - acc: 0.5754 - val_loss: 1.1777 - val_acc: 0.5207\n",
            "Epoch 58/70\n",
            "10250/10250 [==============================] - 24s 2ms/sample - loss: 1.0719 - acc: 0.5681 - val_loss: 1.1708 - val_acc: 0.5267\n",
            "Epoch 59/70\n",
            "10250/10250 [==============================] - 24s 2ms/sample - loss: 1.0684 - acc: 0.5712 - val_loss: 1.1696 - val_acc: 0.5207\n",
            "Epoch 60/70\n",
            "10250/10250 [==============================] - 24s 2ms/sample - loss: 1.0615 - acc: 0.5750 - val_loss: 1.1579 - val_acc: 0.5267\n",
            "Epoch 61/70\n",
            "10250/10250 [==============================] - 24s 2ms/sample - loss: 1.0586 - acc: 0.5856 - val_loss: 1.1523 - val_acc: 0.5327\n",
            "Epoch 62/70\n",
            "10250/10250 [==============================] - 25s 2ms/sample - loss: 1.0698 - acc: 0.5795 - val_loss: 1.1517 - val_acc: 0.5307\n",
            "Epoch 63/70\n",
            "10250/10250 [==============================] - 24s 2ms/sample - loss: 1.0625 - acc: 0.5775 - val_loss: 1.1778 - val_acc: 0.5300\n",
            "Epoch 64/70\n",
            "10250/10250 [==============================] - 24s 2ms/sample - loss: 1.0679 - acc: 0.5748 - val_loss: 1.1694 - val_acc: 0.5260\n",
            "Epoch 65/70\n",
            "10250/10250 [==============================] - 24s 2ms/sample - loss: 1.0524 - acc: 0.5849 - val_loss: 1.1514 - val_acc: 0.5400\n",
            "Epoch 66/70\n",
            "10250/10250 [==============================] - 24s 2ms/sample - loss: 1.0673 - acc: 0.5756 - val_loss: 1.1914 - val_acc: 0.5127\n",
            "Epoch 00066: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9e83ec44a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "tpvU_Xam1rnW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the performance of the LSTM classifier trained with the docs-as-sens approach"
      ]
    },
    {
      "metadata": {
        "id": "X-jt8jsS7lcn",
        "colab_type": "code",
        "outputId": "f9ca8b30-8d03-4e75-ab4d-87ad82d7874a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1986
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate(model,tst,tst_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the accuracy of awl_lstm is 0.657321\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ground-truth</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>321 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     ground-truth  prediction\n",
              "0               2           2\n",
              "1               2           2\n",
              "2               2           2\n",
              "3               2           2\n",
              "4               2           2\n",
              "5               2           2\n",
              "6               2           1\n",
              "7               2           2\n",
              "8               2           2\n",
              "9               2           2\n",
              "10              2           2\n",
              "11              0           2\n",
              "12              0           0\n",
              "13              2           1\n",
              "14              2           2\n",
              "15              2           2\n",
              "16              1           2\n",
              "17              2           0\n",
              "18              2           2\n",
              "19              2           2\n",
              "20              2           2\n",
              "21              2           2\n",
              "22              2           2\n",
              "23              2           2\n",
              "24              2           2\n",
              "25              2           2\n",
              "26              2           2\n",
              "27              2           2\n",
              "28              0           2\n",
              "29              2           0\n",
              "..            ...         ...\n",
              "291             0           1\n",
              "292             0           0\n",
              "293             0           1\n",
              "294             2           2\n",
              "295             1           0\n",
              "296             2           2\n",
              "297             1           0\n",
              "298             0           1\n",
              "299             0           0\n",
              "300             0           1\n",
              "301             1           0\n",
              "302             1           2\n",
              "303             2           2\n",
              "304             2           2\n",
              "305             2           2\n",
              "306             0           2\n",
              "307             0           1\n",
              "308             0           0\n",
              "309             0           2\n",
              "310             0           2\n",
              "311             0           0\n",
              "312             0           0\n",
              "313             2           2\n",
              "314             2           2\n",
              "315             2           2\n",
              "316             2           2\n",
              "317             2           2\n",
              "318             1           1\n",
              "319             0           2\n",
              "320             2           2\n",
              "\n",
              "[321 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    }
  ]
}